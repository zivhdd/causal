{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0690b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a807c57a50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pdb\n",
    "import logging\n",
    "\n",
    "\n",
    "torch.set_printoptions(linewidth=180)\n",
    "\n",
    "FORMAT = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "MAX_CONDITIONS=2\n",
    "MIN_CONDITIONS=0\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3681d79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2001, 0.1001, 0.2993, 0.1404, 0.2202],\n",
       "        [1.0000, 1.0000, 0.1002, 0.6998, 0.1407, 0.4196],\n",
       "        [1.0000, 0.2002, 1.0000, 0.2988, 0.5023, 0.4012],\n",
       "        [1.0000, 0.4678, 0.0999, 1.0000, 0.1407, 0.5716],\n",
       "        [1.0000, 0.2005, 0.3582, 0.2999, 1.0000, 0.6496],\n",
       "        [1.0000, 0.3813, 0.1824, 0.7770, 0.4142, 1.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A single sample consists of 6 events that may occur with different probabilities\n",
    "def generate_samples(size):\n",
    "    base = torch.rand((3,size)) < torch.tensor([1.0, 0.2, 0.1]).unsqueeze(1)\n",
    "    D = torch.rand(size) < (base[1] * 0.5 + 0.2 )\n",
    "    E =  torch.rand(size) < (base[2] * 0.4 + 0.1)\n",
    "    F =  torch.rand(size) < (D*0.5+E*0.5)\n",
    "    return torch.cat((base, D.unsqueeze(0), E.unsqueeze(0), F.unsqueeze(0))).transpose(0,1)\n",
    "\n",
    "\n",
    "def conditional_prob_matrix(samples):\n",
    "    def conditional_prob(samples, idx):\n",
    "        part = samples[samples[:,idx],]\n",
    "        return part.sum(dim=0)/part.shape[0]\n",
    "    return torch.stack([conditional_prob(samples, idx) for idx in range(samples.shape[1])])\n",
    "\n",
    "samples = generate_samples(1000000)\n",
    "conditional_prob_matrix(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91fc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For learning we actually use tensors of the form [5,2,1,7,7,7] ~ P(5 | 2,1)\n",
    "## We need to create a loader that return sample tesnors of that form\n",
    "## P(5 | 2) means that we don't assume that 3 didn't occur- \n",
    "## In  order to learn this semantics when generating these samples we must randomly\n",
    "## drop events that occured \n",
    "\n",
    "MAX_CONDITIONS=2\n",
    "MIN_CONDITIONS=0\n",
    "\n",
    "\n",
    "def shuffle_columns(mat, mask=0):\n",
    "    perms = torch.sort(torch.rand(mat.shape) + mask)[1] + torch.arange(mat.shape[0]).unsqueeze(1) * mat.shape[1]\n",
    "    return mat.flatten()[perms.flatten()].reshape(mat.shape)\n",
    "\n",
    "\n",
    "def create_conditions_target_i(data):\n",
    "    nsamp, ncovs = data.shape\n",
    "    na_vec = torch.tensor([ncovs]*ncovs)\n",
    "    drop_vec = torch.tensor([ncovs+1]*ncovs)\n",
    "    event = torch.randint(0,6, (nsamp,))\n",
    "    target = torch.sum(torch.logical_and(data, event.unsqueeze(1) == torch.arange(ncovs)), dim=1)\n",
    "    id_mat = torch.where(data, torch.arange(ncovs), na_vec)\n",
    "    id_mat = shuffle_columns(id_mat, mask=(id_mat == ncovs))    \n",
    "    nselect =  torch.randint(MIN_CONDITIONS,MAX_CONDITIONS + 1, (id_mat.shape[0],1))\n",
    "    keep = torch.arange(id_mat.shape[1]) < nselect \n",
    "    conditions = torch.where(keep, id_mat, drop_vec)\n",
    "    return torch.concat((event.unsqueeze(1), conditions), dim=1), target.float()\n",
    "\n",
    "def create_conditions_target(data, repeat=5):    \n",
    "    statement, target = zip(*[create_conditions_target_i(data) for idx in range(repeat)])        \n",
    "    return torch.concat(statement, dim=0), torch.concat(target, dim=0)\n",
    "    \n",
    "\n",
    "def create_loader(statement, target, **kwargs):\n",
    "    dataset = torch.utils.data.TensorDataset(statement, target)\n",
    "    loader = torch.utils.data.DataLoader(dataset, **kwargs)    \n",
    "    return loader\n",
    "\n",
    "statement, target = create_conditions_target(samples)\n",
    "loader = create_loader(statement, target, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df22d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[5, 0, 6,  ..., 7, 7, 7],\n",
      "        [4, 0, 4,  ..., 7, 7, 7],\n",
      "        [1, 0, 1,  ..., 7, 7, 7],\n",
      "        ...,\n",
      "        [1, 0, 5,  ..., 7, 7, 7],\n",
      "        [3, 7, 7,  ..., 7, 7, 7],\n",
      "        [0, 3, 0,  ..., 7, 7, 7]]), tensor([0., 1., 1.,  ..., 0., 1., 1.])]\n",
      "torch.Size([1024, 7])\n"
     ]
    }
   ],
   "source": [
    "## This is how the data we use for learning looks like\n",
    "## [2, 0, 7,  ..., 7, 7, 7] ~ F(2 | 0)\n",
    "\n",
    "batch = iter(loader).next()\n",
    "print(batch)\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e4eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Model\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(*[nn.Linear(dim, dim), nn.ReLU()])\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ff(input) + input\n",
    "\n",
    "class CondProbNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, nevents=6, embedding_dim=6, inter_dim=60, nlayers=8):\n",
    "        super().__init__()\n",
    "        self.nconds = MAX_CONDITIONS\n",
    "        self.nevents = nevents\n",
    "        self.none = nevents + 1\n",
    "        self.event_embedding = nn.Embedding(nevents+2, embedding_dim)\n",
    "        self.condition_embedding = nn.Embedding(nevents+2, embedding_dim)\n",
    "        self.ff = nn.Sequential(*(\n",
    "            [nn.Linear(embedding_dim * (1+ self.nconds), inter_dim), nn.ReLU()] +\n",
    "            [Residual(inter_dim) for idx in range(nlayers)] +\n",
    "            [nn.Linear(inter_dim, 1), nn.Sigmoid()]))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if input.shape[1] < self.nconds + 1:\n",
    "            missing = self.nconds + 1  - input.shape[1]\n",
    "            extra = (torch.tensor([self.none] * missing) + torch.zeros(input.shape[0]).unsqueeze(1)).to(input.device)\n",
    "            input = torch.concat((input, extra), dim=1)\n",
    "\n",
    "        event = self.event_embedding(input[:,0:1].int())\n",
    "        cond = self.condition_embedding(input[:,1:self.nconds+1].int())\n",
    "        encoded_input = torch.concat((event.flatten(1), cond.flatten(1)), dim=1)        \n",
    "        return self.ff(encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "062e1262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 15:13:07,434 start training\n",
      "2022-09-20 15:13:07,524 0\n",
      "2022-09-20 15:13:51,219 1\n",
      "2022-09-20 15:14:34,807 2\n",
      "2022-09-20 15:15:18,788 3\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "def train_model(model, loss_fun, optimizer, loader, num_epochs=10, device=None, track=None):\n",
    "    for idx in range(num_epochs):\n",
    "        logging.info(f\"{idx}\")\n",
    "        for data, target in iter(loader):\n",
    "            if device is not None:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_model = model(data)            \n",
    "            loss = loss_fun(y_model.flatten(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if track is not None and not track(idx, model, device):\n",
    "            return\n",
    "\n",
    "def train_ext(model, loader, device_name='cuda:0', batch_size=16, num_epochs=4, lr=0.005, track=None):\n",
    "    #pdb.set_trace()    \n",
    "    logging.info(f\"start training\")\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_model(model=model, loss_fun=nn.BCELoss(), \n",
    "        optimizer=optimizer, loader=loader, \n",
    "        device=device,num_epochs=num_epochs,\n",
    "        track = None)\n",
    "    return model\n",
    "\n",
    "model = CondProbNetwork()\n",
    "cpu = torch.device(\"cpu\")\n",
    "model = train_ext(model, loader).to(cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09674a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conditional probability matrix\n",
      "tensor([[1.0000, 0.0947, 0.0476, 0.1487, 0.0667, 0.0915],\n",
      "        [1.0000, 1.0000, 0.0671, 0.5405, 0.0949, 0.3005],\n",
      "        [1.0000, 0.1408, 1.0000, 0.2057, 0.3381, 0.2274],\n",
      "        [1.0000, 0.3712, 0.0642, 1.0000, 0.0959, 0.4613],\n",
      "        [1.0000, 0.1548, 0.2989, 0.2241, 1.0000, 0.5088],\n",
      "        [1.0000, 0.2996, 0.1262, 0.7303, 0.3490, 1.0000]])\n",
      "Estimate ground truth by sampling\n",
      "tensor([[1.0000, 0.2001, 0.1001, 0.2993, 0.1404, 0.2202],\n",
      "        [1.0000, 1.0000, 0.1002, 0.6998, 0.1407, 0.4196],\n",
      "        [1.0000, 0.2002, 1.0000, 0.2988, 0.5023, 0.4012],\n",
      "        [1.0000, 0.4678, 0.0999, 1.0000, 0.1407, 0.5716],\n",
      "        [1.0000, 0.2005, 0.3582, 0.2999, 1.0000, 0.6496],\n",
      "        [1.0000, 0.3813, 0.1824, 0.7770, 0.4142, 1.0000]])\n",
      "Estimate with the sampling used for learning\n",
      "tensor([[1.0000, 0.0944, 0.0490, 0.1441, 0.0652, 0.0878],\n",
      "        [1.0000, 1.0000, 0.0650, 0.5812, 0.0929, 0.2977],\n",
      "        [1.0000, 0.1384, 1.0000, 0.1957, 0.3889, 0.2623],\n",
      "        [1.0000, 0.3815, 0.0657, 1.0000, 0.0892, 0.4648],\n",
      "        [1.0000, 0.1339, 0.2921, 0.2003, 1.0000, 0.5227],\n",
      "        [1.0000, 0.3175, 0.1448, 0.7602, 0.3835, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Model conditional probability matrix\")\n",
    "print(torch.tensor([model(torch.tensor([[e,c]])) for c in range(6) for e in range(6)]).reshape(6,6))\n",
    "\n",
    "print(\"Estimate ground truth by sampling\")\n",
    "print(conditional_prob_matrix(samples))\n",
    "\n",
    "\n",
    "def sample_prob(e,c):\n",
    "    mask = ((statement[:,0:3] == torch.tensor([e,c,7])).sum(dim=1) == 3)        \n",
    "    return  target[mask].sum()  / target[mask].shape[0] \n",
    "\n",
    "print(\"Estimate with the sampling used for learning\")\n",
    "print(torch.tensor([sample_prob(e,c) for c in range(6) for e in range(6)]).reshape(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "550ac6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few example of conditioning on two events\n",
      "tensor([[0.2981]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.2842]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"A few example of conditioning on two events\")\n",
    "print(model(torch.tensor([[3,4,5,7]])))\n",
    "print(model(torch.tensor([[3,5,4,7]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4ab8514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate value (by sampling)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2713)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The estimate value (by sampling)\")\n",
    "mask = (torch.sum(statement[:,0:4]==torch.tensor([3,4,5,7]), dim=1) == 4)\n",
    "target[mask].sum()/target[mask].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb614baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
